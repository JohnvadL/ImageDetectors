{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgmMMjLeF4SP"
      },
      "source": [
        "## **Load training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGvichG6GtMK"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import dlib\n",
        "from scipy import ndimage\n",
        "import tarfile\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "%mkdir -p /content/data/\n",
        "%cd /content/data"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM1AAmxiHlO_",
        "outputId": "105b4c6a-9fa6-4155-a731-14b2ddc4cc06"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UKsKdonIBMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4dfde4-9af0-4f1d-c542-52f84e724a79"
      },
      "source": [
        "train_path = '/content/drive/My Drive/pretraining/training.tar'\n",
        "val_path = '/content/drive/My Drive/pretraining/validation.tar'\n",
        "\n",
        "# Load training set and validation set\n",
        "for fpath in [train_path, val_path]:\n",
        "  print('Extracting {}...'.format(fpath.split('/')[-1]))\n",
        "  with tarfile.open(fpath) as tar:\n",
        "    tar.extractall()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting training.tar...\n",
            "Extracting validation.tar...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zh6lrAcIyrx"
      },
      "source": [
        "The images of the training set should now be loaded at content/data/training and the images of the validation set at content/data/validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2e7WBa7J46c"
      },
      "source": [
        "## **Building training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTtMH9WIJaEr"
      },
      "source": [
        "class CelebAU(Dataset):\n",
        "  \"\"\"CelebAU dataset labeled by presence of action units (AU)\"\"\"\n",
        "\n",
        "  def __init__(self, train=False, intensity=False, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      - label_csv: Path to the csv file with action unit labels.\n",
        "      - train: training set if True, otherwise validation set\n",
        "      - intensity (bool): labels are intensities (between 0 and 5) rather\n",
        "                          than presence (either 0 or 1).  \n",
        "      - transform: transform applied to an image input\n",
        "    \"\"\"\n",
        "    self.train = train\n",
        "    if train:\n",
        "      label_path = '/content/drive/My Drive/pretraining/train_labels.csv'\n",
        "      self.root_dir = '/content/data/training'\n",
        "    else:\n",
        "      label_path = '/content/drive/My Drive/pretraining/val_labels.csv'\n",
        "      self.root_dir = '/content/data/validation'\n",
        "    self.au_frame = pd.read_csv(label_path, index_col=[0, 1])\n",
        "    if intensity:\n",
        "      self.label_cols = [' AU01_r', ' AU02_r', ' AU04_r', ' AU05_r', ' AU06_r',\n",
        "                         ' AU07_r', ' AU09_r', ' AU10_r', ' AU12_r', ' AU14_r',\n",
        "                         ' AU15_r', ' AU17_r', ' AU20_r', ' AU23_r', ' AU25_r',\n",
        "                         ' AU26_r', ' AU28_c', ' AU45_r']\n",
        "    else:\n",
        "      self.label_cols = [' AU01_c', ' AU02_c', ' AU04_c', ' AU05_c', ' AU06_c',\n",
        "                         ' AU07_c', ' AU09_c', ' AU10_c', ' AU12_c', ' AU14_c',\n",
        "                         ' AU15_c', ' AU17_c', ' AU20_c', ' AU23_c', ' AU25_c',\n",
        "                         ' AU26_c', ' AU28_c', ' AU45_c']\n",
        "    self.intensity = intensity\n",
        "    self.transform = transform\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.au_frame)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Returns a dictionary containing the image and its label if a face is\n",
        "    detected. Otherwise, return None.\n",
        "    \"\"\"\n",
        "    # Get image at idx\n",
        "    image_id = self.au_frame.iloc[idx, 0]\n",
        "    image_path = self.root_dir + '/' + str(image_id).zfill(6) + '.jpg'\n",
        "    image = cv2.imread(image_path)\n",
        "    image = Image.fromarray(image)\n",
        "    \n",
        "    # Get AU labels\n",
        "    aus = self.au_frame.iloc[idx][self.label_cols]\n",
        "    aus = np.array(aus, dtype=float)\n",
        "\n",
        "    if self.transform:\n",
        "      try:\n",
        "        image = self.transform(image)\n",
        "      except ValueError as e:\n",
        "        return None\n",
        "\n",
        "    sample = {'image': image, 'labels': aus}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj05Ac7NvDJx"
      },
      "source": [
        "class align_faces(object):\n",
        "\n",
        "    def __call__(self, image):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Since the images are loaded it PIL\n",
        "        image = np.asarray(image)\n",
        "\n",
        "\n",
        "        # The percentage value of how far in the picture the left eye should be\n",
        "        LEFT_EYE_CORD = (0.25, 0.2)\n",
        "        DIMENSIONS = 64\n",
        "\n",
        "        predictor_path = '/content/drive/My Drive/AUDetector/shape_predictor_68_face_landmarks.dat'\n",
        "        shape_predictor = dlib.shape_predictor(predictor_path)\n",
        "        face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "        \n",
        "\n",
        "        faces = face_detector(image, 0)\n",
        "        print(time.time() - start)\n",
        "\n",
        "        if not faces:\n",
        "          raise ValueError(\"Image has no detectable faces\")\n",
        "\n",
        "        # assumption is made that there is only one\n",
        "        for face in faces:\n",
        "            landmarks = shape_predictor(image, face)\n",
        "\n",
        "            # iterating and converting from points object due to limitations\n",
        "            landmarks = landmarks.parts()\n",
        "            landmarks = self.convert_to_np(landmarks)\n",
        "\n",
        "            # To Gauge Scale\n",
        "            maximum = np.max(landmarks[17:, :], axis=0)\n",
        "            minimum = np.min(landmarks[17:, :], axis=0)\n",
        "\n",
        "            # eye landmarks\n",
        "            left = landmarks[36:42]\n",
        "            right = landmarks[42:48]\n",
        "\n",
        "            # pupil coordinates\n",
        "            left = np.mean(left, axis=0, dtype=np.int)\n",
        "            right = np.mean(right, axis=0, dtype=np.int)\n",
        "\n",
        "            centre = np.vstack((left, right))\n",
        "            centre = np.mean(centre, axis=0, dtype=np.int)\n",
        "\n",
        "            diff = right - left\n",
        "            angle = np.degrees(np.arctan2(diff[1], diff[0]))\n",
        "\n",
        "            # find the length of the face, and use that for our scale\n",
        "            y_scale = maximum[1] - minimum[1]\n",
        "            y_scale = y_scale + 0.2 * y_scale\n",
        "\n",
        "\n",
        "            M = cv2.getRotationMatrix2D((centre[0], centre[1]), angle, DIMENSIONS / y_scale)\n",
        "\n",
        "            # translate the image by eye location\n",
        "            # align the x to the center\n",
        "            #\n",
        "            tX = DIMENSIONS // 2\n",
        "            tY = DIMENSIONS * LEFT_EYE_CORD[1]\n",
        "\n",
        "            M[0, 2] += (tX - centre[0])\n",
        "            M[1, 2] += (tY - centre[1])\n",
        "\n",
        "            image2 = cv2.warpAffine(image, M, (DIMENSIONS, DIMENSIONS),\n",
        "                                    flags=cv2.INTER_CUBIC)\n",
        "          \n",
        "\n",
        "            # convert back to PIL\n",
        "            return Image.fromarray(image2)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_to_np(points):\n",
        "        np_points = np.array([], dtype=np.int)\n",
        "        while points:\n",
        "            point = points.pop()\n",
        "            np_points = np.append(np_points, (point.x, point.y))\n",
        "\n",
        "        np_points = np_points.reshape((-1, 2))\n",
        "        np_points = np.flip(np_points, axis=0)\n",
        "        return np_points"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyUYLgqpNcjN"
      },
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.Grayscale(),\n",
        "     align_faces(),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize(0.5, 0.5)\n",
        "     ]\n",
        ")"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXBZ57VCOYpb"
      },
      "source": [
        "val_set = CelebAU(train=False, transform=transform)\n",
        "val_loader = DataLoader(val_set, batch_size=32, collate_fn=collate_fn, \n",
        "                        shuffle=True, num_workers=2)\n",
        "\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(val_loader)\n",
        "batch = dataiter.next()\n",
        "imshow(torchvision.utils.make_grid(batch['image']))"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrJCLCvHa3wT"
      },
      "source": [
        "# start = time.time()\n",
        "for i_batch, sample_batched in enumerate(val_loader):\n",
        "    # print(i_batch, sample_batched['image'].size(),\n",
        "    #       sample_batched['labels'].size())\n",
        "    if i_batch == 1:\n",
        "      break\n",
        "# print(time.time() - start)\n",
        "\n",
        "# Roughly 50 sec per minibatch -> 62.5 hours per epoch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGyvJeN3vDpQ"
      },
      "source": [
        "##**Transforms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPlrCE34ce7S"
      },
      "source": [
        "## **Defining the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOCKgatZceoH"
      },
      "source": [
        "class AUDetector(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(AUDetector, self).__init__()\n",
        "    self.conv1 = nn.Sequential(nn.Conv2d(1, 64, 3), nn.ReLU(),\n",
        "                               nn.Conv2d(64, 64, 3), nn.ReLU(),\n",
        "                               nn.MaxPool2d(2, stride=2), nn.Dropout2d(p=0.25))\n",
        "\n",
        "    self.conv2 = nn.Sequential(nn.Conv2d(64, 128, 3), nn.ReLU(),\n",
        "                               nn.Conv2d(128, 128, 3), nn.ReLU(),\n",
        "                               nn.MaxPool2d(2, stride=2), nn.Dropout2d(p=0.25))\n",
        "\n",
        "    self.conv3 = nn.Sequential(nn.Conv2d(128, 256, 3), nn.ReLU(),\n",
        "                               nn.Conv2d(256, 256, 3), nn.ReLU(),\n",
        "                               nn.Conv2d(256, 256, 3), nn.ReLU(),\n",
        "                               nn.MaxPool2d(2, stride=2), nn.Dropout2d(p=0.25))\n",
        "\n",
        "    self.conv4 = nn.Sequential(nn.Conv2d(256, 256, 3), nn.ReLU(),\n",
        "                               nn.Conv2d(256, 256, 3), nn.ReLU(),\n",
        "                               nn.Conv2d(256, 256, 3), nn.ReLU(),\n",
        "                               nn.MaxPool2d(2, stride=2), nn.Dropout2d(p=0.25))\n",
        "\n",
        "    self.fc5 = nn.Sequential(nn.Flatten(), nn.Linear(4096, 1024), nn.ReLU(),\n",
        "                             nn.Dropout(p=0.5))\n",
        "\n",
        "    self.fc6 = nn.Sequential(nn.Flatten(), nn.Linear(1024, 1024), nn.ReLU(),\n",
        "                             nn.Dropout(p=0.5))\n",
        "\n",
        "    self.out = nn.Linear(1024, 18)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.fc5(x)\n",
        "    x = self.fc6(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  def predict(self, x):\n",
        "    logits = self.forward(x)\n",
        "    return F.sigmoid(logits) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k47cgyLZfG5c"
      },
      "source": [
        "## **Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRb7_armiJ9-"
      },
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.Grayscale(),\n",
        "     align_faces(),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize(0.5, 0.5)\n",
        "     ]\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRGeBLM_LeqC"
      },
      "source": [
        "def collate_fn(batch):\n",
        "  \"\"\"\n",
        "  Used to process the list of samples to form a batch. Ignores images\n",
        "  where no faces were detected.\n",
        "  \"\"\"\n",
        "  batch = list(filter(lambda x: x is not None, batch))\n",
        "  return torch.utils.data.dataloader.default_collate(batch)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksBo7-a91YRu"
      },
      "source": [
        "def validation_step(convnet, val_loader, criterion, bs):\n",
        "  num_matches = 0.0\n",
        "  total = 0.0\n",
        "  losses = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(val_loader, 0): \n",
        "      imgs, labels = data\n",
        "      imgs, labels = imgs.cuda(), labels.cuda()\n",
        "      outputs = convnet(imgs)\n",
        "\n",
        "      # Compute batch loss\n",
        "      val_loss = criterion(outputs, labels)\n",
        "      losses.append(val_loss.data.item())\n",
        "\n",
        "      # Compute batch accuracy, set probabilities > 0.5 to 1\n",
        "      t = torch.Tensor([0.5])\n",
        "      num_matches += ((F.sigmoid(outputs) > t) == labels).sum()\n",
        "      total += labels.size(0) * 18\n",
        "\n",
        "  val_loss = np.mean(losses)\n",
        "  val_acc = 100 * num_matches / total\n",
        "  return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av_o4NVwfKgq"
      },
      "source": [
        "def train(args, soft_start=False):\n",
        "\n",
        "  convnet = AUDetector()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(convnet.parameters(), args.learn_rate)\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  best_val_loss = 0\n",
        "\n",
        "  if soft_start:\n",
        "    checkpoint = torch.load(args.checkpoint)\n",
        "    convnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    valid_losses = checkpoint['valid_losses']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "\n",
        "  train_set = CelebAU(train=True, args.transform)\n",
        "  train_loader = DataLoader(train_set, args.batch_size, collate_fn=collate_fn, \n",
        "                            shuffle=True, num_workers=args.num_workers)\n",
        "  val_set = CelebAU(train=False, args.transform)\n",
        "  val_loader = DataLoader(val_set, args.batch_size, collate_fn=collate_fn,\n",
        "                          shuffle=True, num_workers=args.num_workers)\n",
        "  \n",
        "  start = time.time()\n",
        "  if torch.cuda.is_available:\n",
        "    print('Running on GPU.') \n",
        "    convnet.cuda()\n",
        "  else:\n",
        "    print('Running on CPU.')\n",
        "  print('Beginning training...')\n",
        "  for epoch in range(args.epochs):\n",
        "\n",
        "    convnet.train() # Set to training mode\n",
        "    losses = []\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "      ###################\n",
        "      # train the model #\n",
        "      ###################\n",
        "      imgs, labels = data\n",
        "      imgs, labels = imgs.cuda(), labels.cuda()\n",
        "      # reset parameter gradients to 0\n",
        "      optimizer.zero_grad()\n",
        "      outputs = convnet(imgs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss.data.item())\n",
        "    \n",
        "    # Evalutate training metrics on epoch\n",
        "    avg_loss = np.mean(losses)\n",
        "    train_losses.append(avg_loss)\n",
        "    time_elapsed = time.time() - start\n",
        "    print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n",
        "            epoch+1, args.epochs, avg_loss, time_elapsed))\n",
        "    \n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "    cnn.eval()\n",
        "    val_loss, val_acc = validation_step(convnet, val_loader, criterion, \n",
        "                                        args.batch_size)\n",
        "    time_elapsed = time.time() - start\n",
        "    valid_losses.append(val_loss)\n",
        "    valid_accs.append(val_acc)\n",
        "    print('Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%, Time(s): %.2f' % (\n",
        "        epoch+1, args.epochs, val_loss, val_acc, time_elapsed))\n",
        "    \n",
        "    if -val_loss >= best_loss:\n",
        "      # SAVE MODEL\n",
        "      best_loss = -val_loss\n",
        "      checkpoint = {\n",
        "          'model_state_dict': convnet.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'train_losses': train_losses,\n",
        "          'valid_losses': valid_losses,\n",
        "          'best_val_loss': best_val_loss\n",
        "      }\n",
        "      torch.save(checkpoint, args.checkpoint)\n",
        "\n",
        "  print(f'Best model achieves accuracy: {best_acc:.4f}')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z89uYXz2vvXu"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'gpu':True,\n",
        "              'transform': transform \n",
        "              'checkpoint':\"/content/drive/My Drive/pretraining/checkpoints\", \n",
        "              'learn_rate':0.3, \n",
        "              'batch_size':100, \n",
        "              'epochs':35, \n",
        "              'num_workers':0,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "train(args, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
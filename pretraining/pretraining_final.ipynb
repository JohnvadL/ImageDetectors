{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Project.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_kRyILy3iwX",
    "outputId": "cf920b4c-e8ff-4e7f-ef71-df0492f29a37"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFBEsMNLCXYH",
    "outputId": "dbb8e5e1-936d-454f-cda8-46400ff29d71"
   },
   "source": [
    "%mkdir -p /content/data/\n",
    "%cd /content/data\n",
    "\n",
    "import tarfile \n",
    "\n",
    "train_path = '/content/drive/My Drive/CSC420/pretraining/training.tar'\n",
    "val_path = '/content/drive/My Drive/CSC420/pretraining/validation.tar'\n",
    "\n",
    "# Load training set and validation set\n",
    "for fpath in [train_path, val_path]:\n",
    "  print('Extracting {}...'.format(fpath.split('/')[-1]))\n",
    "  with tarfile.open(fpath) as tar:\n",
    "    tar.extractall()\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/data\n",
      "Extracting training.tar...\n",
      "Extracting validation.tar...\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dhbajicXxaM3"
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "'''\n",
    "References:\n",
    "Preprocessing steps: https://arxiv.org/pdf/1911.05946.pdf\n",
    "Face landmarks : https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/C\n",
    "'''\n",
    "class align_faces_with_landmarks(object):\n",
    "\n",
    "    def __call__(self, image, landmarks):\n",
    "        # Since the images are loaded it PIL\n",
    "        image = np.asarray(image)\n",
    "\n",
    "        # The percentage value of how far in the picture the left eye should be\n",
    "        LEFT_EYE_CORD = (0.25, 0.2)\n",
    "        DIMENSIONS = 244\n",
    "\n",
    "        landmarks = np.array(landmarks).reshape((5, 2))\n",
    "        # assumption is made that there is only one\n",
    "\n",
    "        # To Gauge Scale\n",
    "        maximum = np.max(landmarks, axis=0)\n",
    "        minimum = np.min(landmarks, axis=0)\n",
    "\n",
    "        # eye landmarks\n",
    "        left = landmarks[:1]\n",
    "        right = landmarks[1:2]\n",
    "\n",
    "        centre = np.vstack((left, right))\n",
    "        centre = np.mean(centre, axis=0, dtype=np.int)\n",
    "\n",
    "        diff = right - left\n",
    "        diff = diff.reshape((2, 1))\n",
    "\n",
    "        angle = np.degrees(np.arctan2(diff[1], diff[0]))\n",
    "\n",
    "        # find the length of the face, and use that for our scale\n",
    "        y_scale = maximum[1] - minimum[1]\n",
    "        y_scale = y_scale + 0.9 * y_scale\n",
    "\n",
    "        M = cv2.getRotationMatrix2D((centre[0], centre[1]), angle, DIMENSIONS / y_scale)\n",
    "\n",
    "        # update translation\n",
    "        t_x = DIMENSIONS // 2\n",
    "        t_y = DIMENSIONS * LEFT_EYE_CORD[1]\n",
    "        M[0, 2] += (t_x - centre[0])\n",
    "        M[1, 2] += (t_y - centre[1])\n",
    "\n",
    "        image2 = cv2.warpAffine(image, M, (DIMENSIONS, DIMENSIONS),\n",
    "                                flags=cv2.INTER_CUBIC)\n",
    "\n",
    "        # convert back to PIL\n",
    "        return Image.fromarray(image2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvNGcI9qw-fq",
    "outputId": "2c857e77-3787-4616-f71e-6fa853412381"
   },
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CelebAU(Dataset):\n",
    "    \"\"\"CelebAU dataset labeled by presence of action units (AU)\"\"\"\n",
    "\n",
    "    def __init__(self, train=False, intensity=False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - label_csv: Path to the csv file with action unit labels.\n",
    "          - train: training set if True, otherwise validation set\n",
    "          - intensity (bool): labels are intensities (between 0 and 5) rather\n",
    "                              than presence (either 0 or 1).\n",
    "          - transform: transform applied to an image input\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        if train:\n",
    "            label_path = '/content/drive/MyDrive/CSC420/pretraining/train_labels.csv'\n",
    "            self.root_dir = '/content/data/training'\n",
    "        else:\n",
    "            label_path = '/content/drive/MyDrive/CSC420/pretraining/val_labels.csv'\n",
    "            self.root_dir = '/content/data/validation'\n",
    "        self.au_frame = pd.read_csv(label_path, index_col=[0, 1])\n",
    "        \n",
    "        if intensity:\n",
    "            self.label_cols = [' AU01_r', ' AU02_r', ' AU04_r', ' AU05_r', ' AU06_r',\n",
    "                               ' AU07_r', ' AU09_r', ' AU10_r', ' AU12_r', ' AU14_r',\n",
    "                               ' AU15_r', ' AU17_r', ' AU20_r', ' AU23_r', ' AU25_r',\n",
    "                               ' AU26_r', ' AU28_r', ' AU45_r']\n",
    "        else:\n",
    "            self.label_cols = [' AU01_c', ' AU02_c', ' AU04_c', ' AU05_c', ' AU06_c',\n",
    "                               ' AU07_c', ' AU09_c', ' AU10_c', ' AU12_c', ' AU14_c',\n",
    "                               ' AU15_c', ' AU17_c', ' AU20_c', ' AU23_c', ' AU25_c',\n",
    "                               ' AU26_c', ' AU28_c', ' AU45_c']\n",
    "\n",
    "        self.landmark_cols = ['name', 'lefteye_x', 'lefteye_y', 'righteye_x', 'righteye_y', 'nose_x', 'nose_y',\n",
    "                              'leftmouth_x', 'leftmouth_y', 'rightmouth_x', 'rightmouth_y']\n",
    "\n",
    "        self.intensity = intensity\n",
    "        self.transform = transform\n",
    "\n",
    "        # code to handle facial landmarks\n",
    "        landmark_path = '/content/drive/MyDrive/CSC420/pretraining/list_landmarks_align_celeba.csv'\n",
    "        self.landmark_frame = pd.read_csv(landmark_path, index_col=[0])\n",
    "        self.align = align_faces_with_landmarks()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.au_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing the image and its label if a face is\n",
    "        detected. Otherwise, return None.\n",
    "        \"\"\"\n",
    "        # Get image at idx\n",
    "        image_id = self.au_frame.iloc[idx, 0]\n",
    "        image_path = self.root_dir + '/' + str(image_id).zfill(6) + '.jpg'\n",
    "        image = cv2.imread(image_path)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        # get landmarks for the specified file\n",
    "        image_id = int(image_id)\n",
    "        landmarks = self.landmark_frame.iloc[image_id - 1]\n",
    "\n",
    "        landmarks = landmarks.tolist()\n",
    "\n",
    "        # Get AU labels\n",
    "        aus = self.au_frame.iloc[idx][self.label_cols]\n",
    "        aus = np.array(aus, dtype=float)\n",
    "\n",
    "        if self.transform:\n",
    "            try:\n",
    "                # torchvision transforms can't take multiple parameters so splitting the transforms here reference:\n",
    "                # https://discuss.pytorch.org/t/t-compose-typeerror-call-takes-2-positional-arguments-but-3-were\n",
    "                # -given/62529\n",
    "                image = self.align(image, landmarks)\n",
    "                image = self.transform(image)\n",
    "\n",
    "            except ValueError:\n",
    "                return None\n",
    "        sample = {'image': image, 'labels': aus}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "  Used to process the list of samples to form a batch. Ignores images\n",
    "  where no faces were detected.\n",
    "  \"\"\"\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\"gray\")\n",
    "    plt.show()\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "OK\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xa0oID0ww26a",
    "outputId": "8b70272a-658c-4d2e-ff7d-abd7c4d98d1d"
   },
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "import torchvision\n",
    "from scipy.io.idl import AttrDict\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def validation_step(convnet, val_loader, criterion, bs):\n",
    "    num_matches = 0.0\n",
    "    total = 0.0\n",
    "    losses = []\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, item in enumerate(val_loader, 0):\n",
    "\n",
    "            labels = item.get(\"labels\")\n",
    "            imgs = item.get(\"image\")\n",
    "            labels = labels.to(device)\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            outputs = convnet(imgs)\n",
    "\n",
    "            # Compute batch loss\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            losses.append(val_loss.data.item())\n",
    "\n",
    "            # Compute batch accuracy, set probabilities > 0.5 to 1\n",
    "            t = torch.Tensor([0.5])\n",
    "            t = t.to(device)\n",
    "            num_matches += ((torch.nn.functional.sigmoid(outputs) > t) == labels).sum()\n",
    "\n",
    "            total += labels.size(0) * 18\n",
    "\n",
    "    val_loss = np.mean(losses)\n",
    "    val_acc = 100 * num_matches / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train(convnet, args, soft_start=False):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    best_loss = -100 \n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    best_model_wts = copy.deepcopy(convnet.state_dict())\n",
    "\n",
    "    train_set = CelebAU(train=True, transform=args.transform)\n",
    "    train_loader = DataLoader(train_set, args.batch_size, collate_fn=collate_fn,\n",
    "                              shuffle=True, num_workers=args.num_workers)\n",
    "    \n",
    "    val_set = CelebAU(train=False, transform=args.transform)\n",
    "    val_loader = DataLoader(val_set, args.batch_size, collate_fn=collate_fn,\n",
    "                            shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "    optimizer = torch.optim.Adam(convnet.parameters(), args.learn_rate)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if args.resume: \n",
    "      print(\"Loading checkpoint\")\n",
    "      state = torch.load(args.checkpoint)\n",
    "      convnet.load_state_dict(state['model_state_dict'])\n",
    "      optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        convnet.train()\n",
    "        losses = []\n",
    "\n",
    "        for _, item in enumerate(train_loader, 0):\n",
    "            \n",
    "            labels = item.get(\"labels\")\n",
    "            imgs = item.get(\"image\")\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = convnet(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        train_losses.append(avg_loss)\n",
    "        time_elapsed = time.time() - start\n",
    "\n",
    "        print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n",
    "            epoch + 1, args.epochs, avg_loss, time_elapsed))\n",
    "        \n",
    "        # Validation \n",
    "        convnet.eval()\n",
    "        val_loss, val_acc = validation_step(convnet, val_loader, criterion,\n",
    "                                            args.batch_size)\n",
    "        time_elapsed = time.time() - start\n",
    "        valid_losses.append(val_loss)\n",
    "        valid_accs.append(val_acc)\n",
    "\n",
    "        print('Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%, Time(s): %.2f' % (\n",
    "            epoch + 1, args.epochs, val_loss, val_acc, time_elapsed))\n",
    "\n",
    "        # Save model\n",
    "        if -val_loss >= best_loss:\n",
    "          print(\"Best Loss: Saving Model\")\n",
    "          best_loss = -val_loss\n",
    "          checkpoint = {\n",
    "                'model_state_dict': convnet.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'valid_losses': valid_losses,\n",
    "                'best_val_loss': best_loss\n",
    "            }\n",
    "          torch.save(checkpoint, args.checkpoint)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Used to process the list of samples to form a batch. Ignores images\n",
    "    where no faces were detected.\n",
    "    \"\"\"\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num = model.fc.in_features\n",
    "    fc =torch.nn.Sequential(\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(512, 256), \n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(256, 18))\n",
    "    \n",
    "    model.fc = fc \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_VGG():\n",
    "\n",
    "    VGG = models.vgg16(pretrained=True)\n",
    "    VGG.classifier.add_module( \"class_facs_1\", torch.nn.Linear(1000, 512 ))\n",
    "    VGG.classifier.add_module(\"class_facs_ReLU2\", torch.nn.LeakyReLU())\n",
    "    VGG.classifier.add_module(\"class_facs_2\",torch.nn.Linear(512, 18 ))\n",
    "    \n",
    "    for name, param in VGG.named_parameters():\n",
    "      if \"class_facs_1\" in name or \"class_facs_2\" in name :\n",
    "        param.requires_grad = True \n",
    "      else: \n",
    "        param.requires_grad = False\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    VGG = VGG.to(device)\n",
    "    return VGG\n",
    "\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "     [torchvision.transforms.ToTensor(), \n",
    "      torchvision.transforms.Normalize((0.4007, 0.4783, 0.6502), (0.1641, 0.1755, 0.2007 ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    'gpu': True,\n",
    "    'transform': transform,\n",
    "    'checkpoint': \"/content/drive/MyDrive/CSC420/pretraining/checkpoint/checkpoint.pt\",\n",
    "    'learn_rate': 0.0001,\n",
    "    'batch_size': 35,\n",
    "    'epochs': 12,\n",
    "    'num_workers': 4,\n",
    "    'resume': False \n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "train(convnet=get_model(), args=args)\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Loss: 0.3435, Time (s): 380\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Val Loss: 0.3089, Val Acc: 85.9%, Time(s): 398.08\n",
      "Best Loss: Saving Model\n",
      "Epoch [2/12], Loss: 0.2876, Time (s): 770\n",
      "Epoch [2/12], Val Loss: 0.2831, Val Acc: 87.1%, Time(s): 786.46\n",
      "Best Loss: Saving Model\n",
      "Epoch [3/12], Loss: 0.2617, Time (s): 1149\n",
      "Epoch [3/12], Val Loss: 0.2804, Val Acc: 87.4%, Time(s): 1166.39\n",
      "Best Loss: Saving Model\n",
      "Epoch [4/12], Loss: 0.2378, Time (s): 1530\n",
      "Epoch [4/12], Val Loss: 0.2775, Val Acc: 87.7%, Time(s): 1546.80\n",
      "Best Loss: Saving Model\n",
      "Epoch [5/12], Loss: 0.2115, Time (s): 1911\n",
      "Epoch [5/12], Val Loss: 0.2897, Val Acc: 87.3%, Time(s): 1927.94\n",
      "Epoch [6/12], Loss: 0.1810, Time (s): 2291\n",
      "Epoch [6/12], Val Loss: 0.3063, Val Acc: 87.4%, Time(s): 2307.67\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}
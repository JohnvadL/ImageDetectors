{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiments.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF7uczVVAo6X"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import dlib\n",
        "from scipy import ndimage\n",
        "import tarfile\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gzJRZICnc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MndTlmgI6lG",
        "outputId": "c675e1ea-a07a-4b5c-82e7-d685bca0b502"
      },
      "source": [
        "%mkdir -p /content/data/\n",
        "%cd /content/data\n",
        "\n",
        "train_path = '/content/drive/My Drive/training/training.tar'\n",
        "val_path = '/content/drive/My Drive/training/validation.tar'\n",
        "test_path = '/content/drive/My Drive/training/test.tar'\n",
        "\n",
        "# Load training set and validation set\n",
        "for fpath in [train_path, val_path, test_path]:\n",
        "  print('Extracting {}...'.format(fpath.split('/')[-1]))\n",
        "  with tarfile.open(fpath) as tar:\n",
        "    tar.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n",
            "Extracting training.tar...\n",
            "Extracting validation.tar...\n",
            "Extracting test.tar...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oYqWgOlYyo_"
      },
      "source": [
        "## **Image Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSTUdbf0DR47"
      },
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class align_faces(object):\n",
        "\n",
        "    def __call__(self, image):\n",
        "\n",
        "        # Since the images are loaded it PIL\n",
        "        image = np.asarray(image)\n",
        "\n",
        "        # The percentage value of how far in the picture the left eye should be\n",
        "        LEFT_EYE_CORD = (0.25, 0.2)\n",
        "        DIMENSIONS = 244\n",
        "\n",
        "        train_folder = '/content/drive/My Drive/training/'\n",
        "        predictor_path = os.path.join(train_folder, \"shape_predictor_5_face_landmarks.dat\")\n",
        "        detector_path = os.path.join(train_folder, \"mmod_human_face_detector.dat\")\n",
        "        shape_predictor = dlib.shape_predictor(predictor_path)\n",
        "        face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "        # changed here for efficiency\n",
        "        # face_detector = dlib.cnn_face_detection_model_v1(detector_path)\n",
        "        faces = face_detector(image)\n",
        "\n",
        "        if not faces:\n",
        "            raise ValueError(\"Image has no detectable faces\")\n",
        "\n",
        "        # assumption is made that there is only one\n",
        "        for face in faces:\n",
        "          # face = face_rect.rect\n",
        "          landmarks = shape_predictor(image, face)\n",
        "          landmarks = landmarks.parts()\n",
        "          landmarks = self.convert_to_np(landmarks)\n",
        "\n",
        "          # To Gauge Scale\n",
        "          maximum = np.max(landmarks, axis=0)\n",
        "          minimum = np.min(landmarks, axis=0)\n",
        "\n",
        "          # eye landmarks\n",
        "          right = landmarks[:2]\n",
        "          left = landmarks[2:4]\n",
        "\n",
        "          left = np.mean(left, axis=0, dtype=np.int)\n",
        "          right = np.mean(right, axis=0, dtype=np.int)\n",
        "\n",
        "          centre = np.vstack((left, right))\n",
        "          centre = np.mean(centre, axis=0, dtype=np.int)\n",
        "\n",
        "          diff = right - left\n",
        "          diff = diff.reshape((2, 1))\n",
        "\n",
        "          angle = np.degrees(np.arctan2(diff[1], diff[0]))\n",
        "\n",
        "          # find the length of the face, and use that for our scale\n",
        "          y_scale = maximum[1] - minimum[1]\n",
        "          y_scale = y_scale + 2.1 * y_scale\n",
        "\n",
        "          M = cv2.getRotationMatrix2D((centre[0], centre[1]), angle, DIMENSIONS / y_scale)\n",
        "\n",
        "          # update translation\n",
        "          t_x = DIMENSIONS // 2\n",
        "          t_y = DIMENSIONS * LEFT_EYE_CORD[1]\n",
        "          M[0, 2] += (t_x - centre[0])\n",
        "          M[1, 2] += (t_y - centre[1])\n",
        "\n",
        "          image2 = cv2.warpAffine(image, M, (DIMENSIONS, DIMENSIONS),\n",
        "                                  flags=cv2.INTER_CUBIC)\n",
        "\n",
        "          # convert back to PIL\n",
        "          return Image.fromarray(image2)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_to_np(points):\n",
        "        np_points = np.array([], dtype=np.int)\n",
        "        while points:\n",
        "            point = points.pop()\n",
        "            np_points = np.append(np_points, (point.x, point.y))\n",
        "\n",
        "        np_points = np_points.reshape((-1, 2))\n",
        "        np_points = np.flip(np_points, axis=0)\n",
        "        return np_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYTxdiBcZFNU"
      },
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.Grayscale(num_output_channels=3), \n",
        "     align_faces(),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.3898, 0.3884, 0.3878), (0.2433, 0.2430, 0.2430))\n",
        "     ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxeiZUHuY3w5"
      },
      "source": [
        "## **Dataset Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVcwvecULX-W"
      },
      "source": [
        "class AUDataset(Dataset):\n",
        "  \"\"\" CK+ dataset labeled by the presence of action units (AU) \"\"\"\n",
        "  \n",
        "  def __init__(self, mode='train', fold=0, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      - label_csv: Path to the csv file with action unit labels.\n",
        "      - mode: one of 'train', 'validate', and 'test'\n",
        "      - fold: one of 0, 1, and 2; determines which folds to use for training\n",
        "      - transform: transform applied to an image input\n",
        "    \"\"\"\n",
        "    try:\n",
        "      path_dict = {\n",
        "          'train': 'training/train_data_',\n",
        "          'validate': 'validation/valid_data_',\n",
        "          'test': 'test/test_data',\n",
        "          'train_full': 'train_full/train_full_data',\n",
        "          'validate_full': 'val_full/val_full_data'\n",
        "      }\n",
        "      fold = fold if ((mode == 'train') or (mode == 'validate')) else ''\n",
        "      self.data_path = '{}{}'.format(path_dict[mode], fold)\n",
        "      self.label_path = '{}{}_labels.csv'.format(path_dict[mode], fold)\n",
        "    except KeyError:\n",
        "      raise ValueError('{} is not a valid mode. Choose train, validation, or test.'.format(mode))\n",
        "    self.au_frame = pd.read_csv(self.label_path)\n",
        "    self.label_cols = ['AU_1', 'AU_2', 'AU_4', 'AU_5', 'AU_6', 'AU_7', \n",
        "                        'AU_9', 'AU_10', 'AU_12', 'AU_14', 'AU_15', 'AU_17',\n",
        "                        'AU_20', 'AU_23/24', 'AU_25', 'AU_26/27', 'AU_28',\n",
        "                        'AU_43']\n",
        "    self.transform = transform\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.au_frame)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get image at idx\n",
        "    image_id = self.au_frame.iloc[idx]['path']\n",
        "    image_path = self.data_path + '/' + image_id\n",
        "    image = cv2.imread(image_path)\n",
        "    try:\n",
        "      image = Image.fromarray(image)\n",
        "    except AttributeError:\n",
        "      raise ValueError('{} not found'.format(image_path))\n",
        "\n",
        "    # Get AU labels\n",
        "    aus = self.au_frame.iloc[idx][self.label_cols]\n",
        "    aus = np.array(aus, dtype=float)\n",
        "\n",
        "    if self.transform:\n",
        "      try:\n",
        "        image = self.transform(image)\n",
        "      except ValueError: # No faces were detected\n",
        "        return None\n",
        "\n",
        "    sample = {'image': image, 'labels': aus}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbw5INxOZBN2"
      },
      "source": [
        "## **Other Useful Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAv2Ad3_Ye_H"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Used to process the list of samples to form a batch. Ignores images\n",
        "    where no faces were detected.\n",
        "    \"\"\"\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\"gray\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9-AEzC7w6Zz"
      },
      "source": [
        "## **Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCMgxKR0w5xI"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "import torchvision\n",
        "from scipy.io.idl import AttrDict\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def validation_step(convnet, val_loader, criterion, bs):\n",
        "  num_matches = 0.0\n",
        "  total = 0.0\n",
        "  losses = []\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for _, item in enumerate(val_loader, 0):\n",
        "\n",
        "          labels = item.get(\"labels\")\n",
        "          imgs = item.get(\"image\")\n",
        "          labels = labels.to(device)\n",
        "          imgs = imgs.to(device)\n",
        "\n",
        "          outputs = convnet(imgs)\n",
        "\n",
        "          # Compute batch loss\n",
        "          val_loss = criterion(outputs, labels)\n",
        "          losses.append(val_loss.data.item())\n",
        "\n",
        "          # Compute batch accuracy, set probabilities > 0.5 to 1\n",
        "          t = torch.Tensor([0.5])\n",
        "          t = t.to(device)\n",
        "          num_matches += ((torch.nn.functional.sigmoid(outputs) > t) == labels).sum()\n",
        "\n",
        "          total += labels.size(0) * 18\n",
        "\n",
        "  val_loss = np.mean(losses)\n",
        "  val_acc = 100 * num_matches / total\n",
        "  return val_loss, val_acc\n",
        "\n",
        "\n",
        "def train(convnet, args):\n",
        "  \"\"\"Training loop for a single validation fold.\"\"\"\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  valid_accs = []\n",
        "  best_loss = -100 \n",
        "  start = time.time()\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  best_model_wts = copy.deepcopy(convnet.state_dict())\n",
        "\n",
        "  train_set = AUDataset(mode='train', fold=args.fold, transform=args.transform)\n",
        "  train_loader = DataLoader(train_set, args.batch_size, collate_fn=collate_fn,\n",
        "                            shuffle=True, num_workers=args.num_workers)\n",
        "  \n",
        "  val_set = AUDataset(mode='validate', fold=args.fold, transform=args.transform)\n",
        "  val_loader = DataLoader(val_set, args.batch_size, collate_fn=collate_fn,\n",
        "                          shuffle=True, num_workers=args.num_workers)\n",
        "\n",
        "  optimizer = torch.optim.Adam(convnet.parameters(), args.learn_rate)\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "  if args.resume: \n",
        "    print(\"Loading checkpoint\")\n",
        "    state = torch.load(args.checkpoint)\n",
        "    convnet.load_state_dict(state['model_state_dict'])\n",
        "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "  for epoch in range(args.epochs):\n",
        "      convnet.train()\n",
        "      losses = []\n",
        "\n",
        "      for _, item in enumerate(train_loader, 0):\n",
        "          \n",
        "          labels = item.get(\"labels\")\n",
        "          imgs = item.get(\"image\")\n",
        "\n",
        "          labels = labels.to(device)\n",
        "          imgs = imgs.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = convnet(imgs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "\n",
        "      avg_loss = np.mean(losses)\n",
        "      train_losses.append(avg_loss)\n",
        "      time_elapsed = time.time() - start\n",
        "\n",
        "      print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n",
        "          epoch + 1, args.epochs, avg_loss, time_elapsed))\n",
        "      \n",
        "      # Validation \n",
        "      convnet.eval()\n",
        "      val_loss, val_acc = validation_step(convnet, val_loader, criterion,\n",
        "                                          args.batch_size)\n",
        "      time_elapsed = time.time() - start\n",
        "      valid_losses.append(val_loss)\n",
        "      valid_accs.append(val_acc)\n",
        "\n",
        "      print('Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%, Time(s): %.2f' % (\n",
        "          epoch + 1, args.epochs, val_loss, val_acc, time_elapsed))\n",
        "\n",
        "      # Save model\n",
        "      if -val_loss >= best_loss:\n",
        "        print(\"Best Loss: Saving Model\")\n",
        "        best_loss = -val_loss\n",
        "        checkpoint = {\n",
        "              'model_state_dict': convnet.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'train_losses': train_losses,\n",
        "              'valid_losses': valid_losses,\n",
        "              'best_val_loss': best_loss\n",
        "          }\n",
        "        torch.save(checkpoint, args.checkpoint)\n",
        "\n",
        "\n",
        "def match_state_dict(state_dict):\n",
        "  \"\"\"\n",
        "  Renames 'fc.0' to 'fc.1' and 'fc.2' to 'fc.3' in order to load pretrained\n",
        "  model weights to new model.\n",
        "  \"\"\"\n",
        "  new_sd = copy.deepcopy(state_dict)\n",
        "  for key in state_dict:\n",
        "    if 'fc' in key:\n",
        "      layer, num, weight = key.split('.')\n",
        "      if int(num) == 0:\n",
        "        new_sd['{}.1.{}'.format(layer, weight)] = new_sd.pop(key)\n",
        "      elif int(num) == 2:\n",
        "        new_sd['{}.3.{}'.format(layer, weight)] = new_sd.pop(key)\n",
        "  return new_sd\n",
        "\n",
        "def get_model(path):\n",
        "  \"\"\"\n",
        "  Returns a model that has already been pretrained on the CelebA dataset.\n",
        "  \"\"\"\n",
        "  model = models.resnet18(pretrained=True)\n",
        "  num = model.fc.in_features\n",
        "  fc = torch.nn.Sequential(\n",
        "      torch.nn.LeakyReLU(),\n",
        "      torch.nn.Linear(512, 256), \n",
        "      torch.nn.LeakyReLU(),\n",
        "      torch.nn.Linear(256, 18))\n",
        "  \n",
        "  model.fc = fc\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  last_cp = torch.load(path)\n",
        "  model_state_dict = match_state_dict(last_cp['model_state_dict'])\n",
        "  model.load_state_dict(model_state_dict)\n",
        "  model = model.to(device)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwk6808wgGA9"
      },
      "source": [
        "## **Scoring Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAJSRvxerYcG"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, auc, precision_recall_curve\n",
        "\n",
        "def get_metrics(model, data):\n",
        "  \"\"\"\n",
        "  Compute the f1 score for each action unit (AU).\n",
        "  \"\"\"\n",
        "  labels = np.array([], dtype=int).reshape(0,18)\n",
        "  preds = np.array([], dtype=int).reshape(0,18)\n",
        "  model.eval() # Set to evaluation mode\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for _, item in enumerate(data, 0):\n",
        "      batch_labels = item.get(\"labels\")\n",
        "      labels = np.vstack([labels, batch_labels])\n",
        "      imgs = item.get(\"image\")\n",
        "      imgs = imgs.to(device)\n",
        "      outputs = model(imgs).cpu().detach()\n",
        "      outputs = torch.sigmoid(outputs).numpy()\n",
        "      preds = np.vstack([preds, outputs])\n",
        "  \n",
        "  # Compute f1 scores\n",
        "  f1_scores = []\n",
        "  pres_scores = []\n",
        "  recall_scores = []\n",
        "  pr_auc_scores = []\n",
        "  for au in range(18):\n",
        "    au_labels = labels[:, au]\n",
        "    au_preds = preds[:, au]\n",
        "    p, r, _ = precision_recall_curve(au_labels, au_preds)\n",
        "    auc_pr = auc(r, p)\n",
        "    if np.isnan(auc_pr):\n",
        "      auc_pr = 0\n",
        "    pr_auc_scores.append(auc_pr)\n",
        "\n",
        "\n",
        "    # Threshold probabilities > 0.4 to 1 for other metrics\n",
        "    if au == 17:\n",
        "      au_preds = (au_preds > 0.05) * 1\n",
        "    else:\n",
        "      au_preds = (au_preds > 0.4) * 1\n",
        "    f1 = f1_score(au_labels, au_preds, zero_division=0); f1_scores.append(f1)\n",
        "    precision = precision_score(au_labels, au_preds, zero_division=0)\n",
        "    pres_scores.append(precision)\n",
        "    recall = recall_score(au_labels, au_preds, zero_division=0)\n",
        "    recall_scores.append(recall)\n",
        "  return pres_scores, recall_scores, f1_scores, pr_auc_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhST8QBB649v"
      },
      "source": [
        "## **First Fold**: Unfreeze only FC layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeufX5T5cUIF",
        "outputId": "40162a9d-12d2-453d-d17c-b59059f9b7de"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint0.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=0, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.754, recall: 0.793, F1: 0.773, PR AUC: 0.851\n",
            "AU_2: precision: 0.744, recall: 0.744, F1: 0.744, PR AUC: 0.825\n",
            "AU_4: precision: 0.744, recall: 0.582, F1: 0.653, PR AUC: 0.754\n",
            "AU_5: precision: 0.818, recall: 0.730, F1: 0.771, PR AUC: 0.820\n",
            "AU_6: precision: 0.696, recall: 0.800, F1: 0.744, PR AUC: 0.813\n",
            "AU_7: precision: 0.600, recall: 0.417, F1: 0.492, PR AUC: 0.569\n",
            "AU_9: precision: 0.917, recall: 0.917, F1: 0.917, PR AUC: 0.934\n",
            "AU_10: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.087\n",
            "AU_12: precision: 0.822, recall: 0.902, F1: 0.860, PR AUC: 0.930\n",
            "AU_14: precision: 0.500, recall: 0.556, F1: 0.526, PR AUC: 0.400\n",
            "AU_15: precision: 0.500, recall: 0.370, F1: 0.426, PR AUC: 0.533\n",
            "AU_17: precision: 0.781, recall: 0.794, F1: 0.787, PR AUC: 0.878\n",
            "AU_20: precision: 0.654, recall: 0.739, F1: 0.694, PR AUC: 0.774\n",
            "AU_23/24: precision: 0.500, recall: 0.296, F1: 0.372, PR AUC: 0.410\n",
            "AU_25: precision: 0.911, recall: 0.911, F1: 0.911, PR AUC: 0.976\n",
            "AU_26/27: precision: 0.850, recall: 0.756, F1: 0.800, PR AUC: 0.891\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.000\n",
            "AU_43: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.075\n",
            "mean precision: 0.599, mean recall: 0.573, mean F1: 0.582, mean PR-AUC: 0.640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSIGGGU8E6xI",
        "outputId": "c8fd0b56-4f66-4a91-8dfd-4db235b960c0"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 1,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint3.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if 'fc' in name:\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.6332, Time (s): 176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.4333, Val Acc: 83.8%, Time(s): 255.21\n",
            "Best Loss: Saving Model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f2cfc05e0f0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f2cfc05e0f0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f2cfc05e0f0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f2cfc05e0f0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2/12], Loss: 0.3856, Time (s): 421\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f2cfc05e0f0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f2cfc05e0f0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f2cfc05e0f0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2/12], Val Loss: 0.3281, Val Acc: 88.6%, Time(s): 495.91\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.2801, Time (s): 658\n",
            "Epoch [3/12], Val Loss: 0.2764, Val Acc: 90.1%, Time(s): 731.43\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.2473, Time (s): 895\n",
            "Epoch [4/12], Val Loss: 0.2707, Val Acc: 90.9%, Time(s): 971.19\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.2353, Time (s): 1141\n",
            "Epoch [5/12], Val Loss: 0.2478, Val Acc: 91.4%, Time(s): 1218.39\n",
            "Best Loss: Saving Model\n",
            "Epoch [6/12], Loss: 0.2269, Time (s): 1381\n",
            "Epoch [6/12], Val Loss: 0.2402, Val Acc: 91.6%, Time(s): 1455.59\n",
            "Best Loss: Saving Model\n",
            "Epoch [7/12], Loss: 0.2219, Time (s): 1617\n",
            "Epoch [7/12], Val Loss: 0.2422, Val Acc: 92.0%, Time(s): 1690.29\n",
            "Epoch [8/12], Loss: 0.2144, Time (s): 1852\n",
            "Epoch [8/12], Val Loss: 0.2259, Val Acc: 91.9%, Time(s): 1925.24\n",
            "Best Loss: Saving Model\n",
            "Epoch [9/12], Loss: 0.2084, Time (s): 2087\n",
            "Epoch [9/12], Val Loss: 0.2218, Val Acc: 92.2%, Time(s): 2166.73\n",
            "Best Loss: Saving Model\n",
            "Epoch [10/12], Loss: 0.2026, Time (s): 2345\n",
            "Epoch [10/12], Val Loss: 0.2125, Val Acc: 92.3%, Time(s): 2428.45\n",
            "Best Loss: Saving Model\n",
            "Epoch [11/12], Loss: 0.2048, Time (s): 2612\n",
            "Epoch [11/12], Val Loss: 0.2182, Val Acc: 92.4%, Time(s): 2695.63\n",
            "Epoch [12/12], Loss: 0.1977, Time (s): 2879\n",
            "Epoch [12/12], Val Loss: 0.2129, Val Acc: 92.4%, Time(s): 2964.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssaosE1-Fd-o",
        "outputId": "eaa2802e-1efe-4e44-e0e5-a84a32a42f63"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 2,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint4.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if 'fc' in name:\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.6709, Time (s): 177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.4006, Val Acc: 85.5%, Time(s): 270.76\n",
            "Best Loss: Saving Model\n",
            "Epoch [2/12], Loss: 0.4168, Time (s): 449\n",
            "Epoch [2/12], Val Loss: 0.2792, Val Acc: 89.8%, Time(s): 542.40\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.3023, Time (s): 720\n",
            "Epoch [3/12], Val Loss: 0.2375, Val Acc: 91.1%, Time(s): 814.81\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.2677, Time (s): 994\n",
            "Epoch [4/12], Val Loss: 0.2235, Val Acc: 92.0%, Time(s): 1089.30\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.2619, Time (s): 1267\n",
            "Epoch [5/12], Val Loss: 0.2167, Val Acc: 92.3%, Time(s): 1357.81\n",
            "Best Loss: Saving Model\n",
            "Epoch [6/12], Loss: 0.2390, Time (s): 1528\n",
            "Epoch [6/12], Val Loss: 0.2109, Val Acc: 91.9%, Time(s): 1615.82\n",
            "Best Loss: Saving Model\n",
            "Epoch [7/12], Loss: 0.2363, Time (s): 1780\n",
            "Epoch [7/12], Val Loss: 0.2041, Val Acc: 92.6%, Time(s): 1866.33\n",
            "Best Loss: Saving Model\n",
            "Epoch [8/12], Loss: 0.2252, Time (s): 2026\n",
            "Epoch [8/12], Val Loss: 0.2030, Val Acc: 92.7%, Time(s): 2112.04\n",
            "Best Loss: Saving Model\n",
            "Epoch [9/12], Loss: 0.2328, Time (s): 2285\n",
            "Epoch [9/12], Val Loss: 0.1978, Val Acc: 92.5%, Time(s): 2377.80\n",
            "Best Loss: Saving Model\n",
            "Epoch [10/12], Loss: 0.2283, Time (s): 2552\n",
            "Epoch [10/12], Val Loss: 0.1949, Val Acc: 93.0%, Time(s): 2640.14\n",
            "Best Loss: Saving Model\n",
            "Epoch [11/12], Loss: 0.2257, Time (s): 2803\n",
            "Epoch [11/12], Val Loss: 0.1933, Val Acc: 93.0%, Time(s): 2887.58\n",
            "Best Loss: Saving Model\n",
            "Epoch [12/12], Loss: 0.2025, Time (s): 3050\n",
            "Epoch [12/12], Val Loss: 0.1900, Val Acc: 93.1%, Time(s): 3135.34\n",
            "Best Loss: Saving Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UgI_rSxcTtn",
        "outputId": "3826d074-b359-49b6-f685-c2f491facb0b"
      },
      "source": [
        "# Validation Loss\n",
        "print(f'Epoch 1: {np.mean([0.4703, 0.4333, 0.4006])} +- {np.std([0.4703, 0.4333, 0.4006])} ')\n",
        "print(f'Epoch 2: {np.mean([0.3563, 0.3281, 0.2792])} +- {np.std([0.3563, 0.3281, 0.2792])} ')\n",
        "print(f'Epoch 3: {np.mean([0.2828, 0.2764, 0.2375])} +- {np.std([0.2828, 0.2764, 0.2375])} ')\n",
        "print(f'Epoch 4: {np.mean([0.2599, 0.2707, 0.2235])} +- {np.std([0.2599, 0.2707, 0.2235])} ')\n",
        "print(f'Epoch 5: {np.mean([0.2531, 0.2478, 0.2167])} +- {np.std([0.2531, 0.2478, 0.2167])} ')\n",
        "print(f'Epoch 6: {np.mean([0.2421, 0.2402, 0.2109])} +- {np.std([0.2421, 0.2402, 0.2109])} ')\n",
        "print(f'Epoch 7: {np.mean([0.2360, 0.2422, 0.2041])} +- {np.std([0.2360, 0.2422, 0.2041])} ')\n",
        "print(f'Epoch 8: {np.mean([0.2309, 0.2259, 0.2030])} +- {np.std([0.2309, 0.2259, 0.2030])} ')\n",
        "print(f'Epoch 9: {np.mean([0.2309, 0.2218, 0.1978])} +- {np.std([0.2309, 0.2218, 0.1978])} ')\n",
        "print(f'Epoch 10: {np.mean([0.2255, 0.2125, 0.1949])} +- {np.std([0.2255, 0.2125, 0.1949])} ')\n",
        "print(f'Epoch 11: {np.mean([0.2212, 0.2182, 0.1933])} +- {np.std([0.2212, 0.2182, 0.1933])} ')\n",
        "print(f'Epoch 12: {np.mean([0.2176, 0.2129, 0.1900])} +- {np.std([0.2176, 0.2129, 0.1900])} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 0.43473333333333336 +- 0.028472950126196765 \n",
            "Epoch 2: 0.3212 +- 0.03185184453057625 \n",
            "Epoch 3: 0.2655666666666667 +- 0.02001738133611776 \n",
            "Epoch 4: 0.2513666666666667 +- 0.02019196759990357 \n",
            "Epoch 5: 0.2392 +- 0.016056359072550243 \n",
            "Epoch 6: 0.23106666666666667 +- 0.014281067498226064 \n",
            "Epoch 7: 0.2274333333333333 +- 0.016692180471373077 \n",
            "Epoch 8: 0.21993333333333331 +- 0.012146421512344914 \n",
            "Epoch 9: 0.21683333333333332 +- 0.013961932371830034 \n",
            "Epoch 10: 0.21096666666666666 +- 0.012539360253041448 \n",
            "Epoch 11: 0.2109 +- 0.012505198918849715 \n",
            "Epoch 12: 0.20683333333333334 +- 0.012056625656551759 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QQNacq1eB5L",
        "outputId": "d539746e-d707-4791-8dd2-fc92346cf075"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint3.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=1, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.765, recall: 0.736, F1: 0.750, PR AUC: 0.770\n",
            "AU_2: precision: 0.829, recall: 0.829, F1: 0.829, PR AUC: 0.866\n",
            "AU_4: precision: 0.656, recall: 0.824, F1: 0.730, PR AUC: 0.808\n",
            "AU_5: precision: 0.704, recall: 0.792, F1: 0.745, PR AUC: 0.699\n",
            "AU_6: precision: 0.640, recall: 0.500, F1: 0.561, PR AUC: 0.710\n",
            "AU_7: precision: 0.500, recall: 0.500, F1: 0.500, PR AUC: 0.478\n",
            "AU_9: precision: 0.895, recall: 0.944, F1: 0.919, PR AUC: 0.948\n",
            "AU_10: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.114\n",
            "AU_12: precision: 0.938, recall: 0.833, F1: 0.882, PR AUC: 0.971\n",
            "AU_14: precision: 0.167, recall: 0.125, F1: 0.143, PR AUC: 0.159\n",
            "AU_15: precision: 0.667, recall: 0.316, F1: 0.429, PR AUC: 0.496\n",
            "AU_17: precision: 0.797, recall: 0.839, F1: 0.817, PR AUC: 0.910\n",
            "AU_20: precision: 0.583, recall: 0.636, F1: 0.609, PR AUC: 0.556\n",
            "AU_23/24: precision: 0.615, recall: 0.296, F1: 0.400, PR AUC: 0.553\n",
            "AU_25: precision: 0.952, recall: 0.878, F1: 0.913, PR AUC: 0.959\n",
            "AU_26/27: precision: 0.771, recall: 0.794, F1: 0.783, PR AUC: 0.876\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.021\n",
            "AU_43: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.014\n",
            "mean precision: 0.582, mean recall: 0.547, mean F1: 0.556, mean PR-AUC: 0.606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77Tixw38ewGk",
        "outputId": "b3f67c4d-c78e-4658-80ef-ccfcda5ad326"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint4.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=2, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.722, recall: 0.765, F1: 0.743, PR AUC: 0.834\n",
            "AU_2: precision: 0.784, recall: 0.935, F1: 0.853, PR AUC: 0.942\n",
            "AU_4: precision: 0.844, recall: 0.915, F1: 0.878, PR AUC: 0.907\n",
            "AU_5: precision: 0.654, recall: 0.586, F1: 0.618, PR AUC: 0.543\n",
            "AU_6: precision: 0.700, recall: 0.761, F1: 0.729, PR AUC: 0.826\n",
            "AU_7: precision: 0.582, recall: 0.744, F1: 0.653, PR AUC: 0.669\n",
            "AU_9: precision: 0.893, recall: 0.893, F1: 0.893, PR AUC: 0.972\n",
            "AU_10: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.053\n",
            "AU_12: precision: 0.950, recall: 0.864, F1: 0.905, PR AUC: 0.962\n",
            "AU_14: precision: 0.333, recall: 0.077, F1: 0.125, PR AUC: 0.240\n",
            "AU_15: precision: 0.333, recall: 0.588, F1: 0.426, PR AUC: 0.403\n",
            "AU_17: precision: 0.792, recall: 0.905, F1: 0.844, PR AUC: 0.929\n",
            "AU_20: precision: 0.667, recall: 0.538, F1: 0.596, PR AUC: 0.639\n",
            "AU_23/24: precision: 0.314, recall: 0.500, F1: 0.386, PR AUC: 0.303\n",
            "AU_25: precision: 0.901, recall: 0.867, F1: 0.883, PR AUC: 0.979\n",
            "AU_26/27: precision: 0.805, recall: 0.805, F1: 0.805, PR AUC: 0.914\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.000\n",
            "AU_43: precision: 0.286, recall: 0.667, F1: 0.400, PR AUC: 0.237\n",
            "mean precision: 0.587, mean recall: 0.634, mean F1: 0.597, mean PR-AUC: 0.631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spzz6rVw7DQr"
      },
      "source": [
        "## **Second Fold**: Unfreeze FC layers and last residual block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd2kpju1fqpS",
        "outputId": "596ab582-f7e5-4997-c078-0ae46753a639"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 1,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint1.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if ('fc' in name) or ('layer4' in name):\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.5048, Time (s): 174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.2461, Val Acc: 91.2%, Time(s): 253.99\n",
            "Best Loss: Saving Model\n",
            "Epoch [2/12], Loss: 0.1804, Time (s): 435\n",
            "Epoch [2/12], Val Loss: 0.1941, Val Acc: 92.9%, Time(s): 516.98\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.1325, Time (s): 696\n",
            "Epoch [3/12], Val Loss: 0.1713, Val Acc: 93.4%, Time(s): 777.60\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.1098, Time (s): 955\n",
            "Epoch [4/12], Val Loss: 0.1712, Val Acc: 93.6%, Time(s): 1035.48\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.0883, Time (s): 1211\n",
            "Epoch [5/12], Val Loss: 0.1701, Val Acc: 93.5%, Time(s): 1290.34\n",
            "Best Loss: Saving Model\n",
            "Epoch [6/12], Loss: 0.0729, Time (s): 1469\n",
            "Epoch [6/12], Val Loss: 0.1669, Val Acc: 93.4%, Time(s): 1550.10\n",
            "Best Loss: Saving Model\n",
            "Epoch [7/12], Loss: 0.0623, Time (s): 1728\n",
            "Epoch [7/12], Val Loss: 0.1667, Val Acc: 93.8%, Time(s): 1809.69\n",
            "Best Loss: Saving Model\n",
            "Epoch [8/12], Loss: 0.0502, Time (s): 1986\n",
            "Epoch [8/12], Val Loss: 0.1657, Val Acc: 93.6%, Time(s): 2066.90\n",
            "Best Loss: Saving Model\n",
            "Epoch [9/12], Loss: 0.0390, Time (s): 2244\n",
            "Epoch [9/12], Val Loss: 0.1689, Val Acc: 93.7%, Time(s): 2324.39\n",
            "Epoch [10/12], Loss: 0.0327, Time (s): 2500\n",
            "Epoch [10/12], Val Loss: 0.1614, Val Acc: 93.9%, Time(s): 2580.89\n",
            "Best Loss: Saving Model\n",
            "Epoch [11/12], Loss: 0.0313, Time (s): 2756\n",
            "Epoch [11/12], Val Loss: 0.1683, Val Acc: 93.7%, Time(s): 2834.74\n",
            "Epoch [12/12], Loss: 0.0234, Time (s): 3004\n",
            "Epoch [12/12], Val Loss: 0.1779, Val Acc: 93.8%, Time(s): 3082.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srjNTHb1FyNn",
        "outputId": "b7f38f7d-8c2e-421c-a1f3-976b8dadfb58"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 0,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint5.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if ('fc' in name) or ('layer4' in name):\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.5009, Time (s): 174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.2303, Val Acc: 91.9%, Time(s): 261.50\n",
            "Best Loss: Saving Model\n",
            "Epoch [2/12], Loss: 0.1760, Time (s): 428\n",
            "Epoch [2/12], Val Loss: 0.1968, Val Acc: 93.3%, Time(s): 512.83\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.1299, Time (s): 676\n",
            "Epoch [3/12], Val Loss: 0.1796, Val Acc: 93.7%, Time(s): 760.43\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.1044, Time (s): 923\n",
            "Epoch [4/12], Val Loss: 0.1735, Val Acc: 94.0%, Time(s): 1006.92\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.0858, Time (s): 1170\n",
            "Epoch [5/12], Val Loss: 0.1727, Val Acc: 93.8%, Time(s): 1255.94\n",
            "Best Loss: Saving Model\n",
            "Epoch [6/12], Loss: 0.0768, Time (s): 1421\n",
            "Epoch [6/12], Val Loss: 0.1723, Val Acc: 93.6%, Time(s): 1505.89\n",
            "Best Loss: Saving Model\n",
            "Epoch [7/12], Loss: 0.0610, Time (s): 1674\n",
            "Epoch [7/12], Val Loss: 0.1727, Val Acc: 93.7%, Time(s): 1759.58\n",
            "Epoch [8/12], Loss: 0.0488, Time (s): 1928\n",
            "Epoch [8/12], Val Loss: 0.1726, Val Acc: 93.8%, Time(s): 2012.77\n",
            "Epoch [9/12], Loss: 0.0412, Time (s): 2177\n",
            "Epoch [9/12], Val Loss: 0.1768, Val Acc: 93.6%, Time(s): 2261.91\n",
            "Epoch [10/12], Loss: 0.0324, Time (s): 2427\n",
            "Epoch [10/12], Val Loss: 0.1746, Val Acc: 94.0%, Time(s): 2512.07\n",
            "Epoch [11/12], Loss: 0.0262, Time (s): 2677\n",
            "Epoch [11/12], Val Loss: 0.1776, Val Acc: 93.9%, Time(s): 2761.33\n",
            "Epoch [12/12], Loss: 0.0241, Time (s): 2926\n",
            "Epoch [12/12], Val Loss: 0.1792, Val Acc: 93.8%, Time(s): 3010.05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkFmgFIpF0Or",
        "outputId": "393bf837-bd7d-4894-dec0-1d5b0d0717a0"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 2,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint6.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if ('fc' in name) or ('layer4' in name):\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.5173, Time (s): 161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.2095, Val Acc: 92.4%, Time(s): 247.13\n",
            "Best Loss: Saving Model\n",
            "Epoch [2/12], Loss: 0.1848, Time (s): 409\n",
            "Epoch [2/12], Val Loss: 0.1701, Val Acc: 93.5%, Time(s): 495.31\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.1363, Time (s): 659\n",
            "Epoch [3/12], Val Loss: 0.1585, Val Acc: 94.4%, Time(s): 746.08\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.1123, Time (s): 912\n",
            "Epoch [4/12], Val Loss: 0.1581, Val Acc: 94.1%, Time(s): 998.92\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.0920, Time (s): 1165\n",
            "Epoch [5/12], Val Loss: 0.1522, Val Acc: 94.3%, Time(s): 1253.69\n",
            "Best Loss: Saving Model\n",
            "Epoch [6/12], Loss: 0.0807, Time (s): 1421\n",
            "Epoch [6/12], Val Loss: 0.1523, Val Acc: 94.3%, Time(s): 1510.05\n",
            "Epoch [7/12], Loss: 0.0674, Time (s): 1679\n",
            "Epoch [7/12], Val Loss: 0.1514, Val Acc: 94.2%, Time(s): 1769.44\n",
            "Best Loss: Saving Model\n",
            "Epoch [8/12], Loss: 0.0573, Time (s): 1941\n",
            "Epoch [8/12], Val Loss: 0.1524, Val Acc: 94.2%, Time(s): 2032.03\n",
            "Epoch [9/12], Loss: 0.0449, Time (s): 2204\n",
            "Epoch [9/12], Val Loss: 0.1490, Val Acc: 94.3%, Time(s): 2294.27\n",
            "Best Loss: Saving Model\n",
            "Epoch [10/12], Loss: 0.0387, Time (s): 2467\n",
            "Epoch [10/12], Val Loss: 0.1525, Val Acc: 94.2%, Time(s): 2558.74\n",
            "Epoch [11/12], Loss: 0.0345, Time (s): 2732\n",
            "Epoch [11/12], Val Loss: 0.1553, Val Acc: 94.3%, Time(s): 2823.56\n",
            "Epoch [12/12], Loss: 0.0323, Time (s): 2998\n",
            "Epoch [12/12], Val Loss: 0.1575, Val Acc: 94.4%, Time(s): 3089.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEaXxccnlEvr",
        "outputId": "345e084d-a2eb-4535-a06c-14daf7c4ed4d"
      },
      "source": [
        "print(f'Epoch 1: {np.mean([0.2461, 0.2303, 0.2095])} +- {np.std([0.2461, 0.2303, 0.2095])} ')\n",
        "print(f'Epoch 2: {np.mean([0.1941, 0.1968, 0.1701])} +- {np.std([0.1941, 0.1968, 0.1701])} ')\n",
        "print(f'Epoch 3: {np.mean([0.1713, 0.1796, 0.1585])} +- {np.std([0.1713, 0.1796, 0.1585])} ')\n",
        "print(f'Epoch 4: {np.mean([0.1712, 0.1735, 0.1581])} +- {np.std([0.1712, 0.1735, 0.1581])} ')\n",
        "print(f'Epoch 5: {np.mean([0.1701, 0.1727, 0.1522])} +- {np.std([0.1701, 0.1727, 0.1522])} ')\n",
        "print(f'Epoch 6: {np.mean([0.1669, 0.1723, 0.1523])} +- {np.std([0.1669, 0.1723, 0.1523])} ')\n",
        "print(f'Epoch 7: {np.mean([0.1667, 0.1727, 0.1514])} +- {np.std([0.1667, 0.1727, 0.1514])} ')\n",
        "print(f'Epoch 8: {np.mean([0.1657, 0.1726, 0.1524])} +- {np.std([0.1657, 0.1726, 0.1524])} ')\n",
        "print(f'Epoch 9: {np.mean([0.1689, 0.1768, 0.1490])} +- {np.std([0.1689, 0.1768, 0.1490])} ')\n",
        "print(f'Epoch 10: {np.mean([0.1614, 0.1746, 0.1525])} +- {np.std([0.1614, 0.1746, 0.1525])} ')\n",
        "print(f'Epoch 11: {np.mean([0.1683, 0.1776, 0.1553])} +- {np.std([0.1683, 0.1776, 0.1553])} ')\n",
        "print(f'Epoch 12: {np.mean([0.1779, 0.1792, 0.1575])} +- {np.std([0.1779, 0.1792, 0.1575])} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 0.22863333333333336 +- 0.01498829172684096 \n",
            "Epoch 2: 0.18700000000000003 +- 0.012000833304400157 \n",
            "Epoch 3: 0.16979999999999998 +- 0.0086790936546777 \n",
            "Epoch 4: 0.1676 +- 0.006782821438506742 \n",
            "Epoch 5: 0.165 +- 0.00911299438530863 \n",
            "Epoch 6: 0.16383333333333333 +- 0.008448010942753862 \n",
            "Epoch 7: 0.1636 +- 0.008967719888578137 \n",
            "Epoch 8: 0.16356666666666667 +- 0.008383449303372818 \n",
            "Epoch 9: 0.16490000000000002 +- 0.01169643820428539 \n",
            "Epoch 10: 0.1628333333333333 +- 0.009079035680560406 \n",
            "Epoch 11: 0.16706666666666667 +- 0.009145612184114431 \n",
            "Epoch 12: 0.17153333333333332 +- 0.009937247550951364 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHrsC4K3kttQ",
        "outputId": "ae27cb0b-0ecc-45ca-d024-6fd7c11da552"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint5.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=0, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.895, recall: 0.879, F1: 0.887, PR AUC: 0.917\n",
            "AU_2: precision: 0.750, recall: 0.769, F1: 0.759, PR AUC: 0.888\n",
            "AU_4: precision: 0.725, recall: 0.673, F1: 0.698, PR AUC: 0.796\n",
            "AU_5: precision: 0.909, recall: 0.811, F1: 0.857, PR AUC: 0.932\n",
            "AU_6: precision: 0.732, recall: 0.750, F1: 0.741, PR AUC: 0.831\n",
            "AU_7: precision: 0.655, recall: 0.528, F1: 0.585, PR AUC: 0.575\n",
            "AU_9: precision: 0.957, recall: 0.917, F1: 0.936, PR AUC: 0.962\n",
            "AU_10: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.073\n",
            "AU_12: precision: 0.792, recall: 0.927, F1: 0.854, PR AUC: 0.942\n",
            "AU_14: precision: 0.545, recall: 0.667, F1: 0.600, PR AUC: 0.733\n",
            "AU_15: precision: 0.576, recall: 0.704, F1: 0.633, PR AUC: 0.752\n",
            "AU_17: precision: 0.836, recall: 0.889, F1: 0.862, PR AUC: 0.934\n",
            "AU_20: precision: 0.857, recall: 0.783, F1: 0.818, PR AUC: 0.914\n",
            "AU_23/24: precision: 0.625, recall: 0.370, F1: 0.465, PR AUC: 0.607\n",
            "AU_25: precision: 0.960, recall: 0.941, F1: 0.950, PR AUC: 0.984\n",
            "AU_26/27: precision: 0.864, recall: 0.844, F1: 0.854, PR AUC: 0.913\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.000\n",
            "AU_43: precision: 0.083, recall: 0.333, F1: 0.133, PR AUC: 0.090\n",
            "mean precision: 0.653, mean recall: 0.655, mean F1: 0.646, mean PR-AUC: 0.713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdd3sSHZgElu",
        "outputId": "ec9db965-20e4-464b-b72c-61bc56fb7438"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint1.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=1, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.745, recall: 0.774, F1: 0.759, PR AUC: 0.889\n",
            "AU_2: precision: 0.744, recall: 0.829, F1: 0.784, PR AUC: 0.893\n",
            "AU_4: precision: 0.703, recall: 0.882, F1: 0.783, PR AUC: 0.825\n",
            "AU_5: precision: 0.733, recall: 0.917, F1: 0.815, PR AUC: 0.854\n",
            "AU_6: precision: 0.594, recall: 0.594, F1: 0.594, PR AUC: 0.769\n",
            "AU_7: precision: 0.618, recall: 0.618, F1: 0.618, PR AUC: 0.517\n",
            "AU_9: precision: 0.941, recall: 0.889, F1: 0.914, PR AUC: 0.950\n",
            "AU_10: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.216\n",
            "AU_12: precision: 0.886, recall: 0.861, F1: 0.873, PR AUC: 0.957\n",
            "AU_14: precision: 0.429, recall: 0.375, F1: 0.400, PR AUC: 0.386\n",
            "AU_15: precision: 0.893, recall: 0.658, F1: 0.758, PR AUC: 0.898\n",
            "AU_17: precision: 0.926, recall: 0.893, F1: 0.909, PR AUC: 0.962\n",
            "AU_20: precision: 0.789, recall: 0.682, F1: 0.732, PR AUC: 0.751\n",
            "AU_23/24: precision: 0.762, recall: 0.593, F1: 0.667, PR AUC: 0.786\n",
            "AU_25: precision: 0.925, recall: 0.956, F1: 0.940, PR AUC: 0.962\n",
            "AU_26/27: precision: 0.811, recall: 0.882, F1: 0.845, PR AUC: 0.939\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.016\n",
            "AU_43: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.033\n",
            "mean precision: 0.639, mean recall: 0.633, mean F1: 0.633, mean PR-AUC: 0.700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nTzddLRk4xD",
        "outputId": "8e2ee7c7-3206-4656-de3b-537158cf7b62"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint6.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=2, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.727, recall: 0.941, F1: 0.821, PR AUC: 0.913\n",
            "AU_2: precision: 0.909, recall: 0.968, F1: 0.937, PR AUC: 0.984\n",
            "AU_4: precision: 0.849, recall: 0.873, F1: 0.861, PR AUC: 0.919\n",
            "AU_5: precision: 0.750, recall: 0.621, F1: 0.679, PR AUC: 0.712\n",
            "AU_6: precision: 0.712, recall: 0.804, F1: 0.755, PR AUC: 0.874\n",
            "AU_7: precision: 0.660, recall: 0.814, F1: 0.729, PR AUC: 0.705\n",
            "AU_9: precision: 0.963, recall: 0.929, F1: 0.945, PR AUC: 0.988\n",
            "AU_10: precision: 0.667, recall: 0.400, F1: 0.500, PR AUC: 0.208\n",
            "AU_12: precision: 0.950, recall: 0.864, F1: 0.905, PR AUC: 0.953\n",
            "AU_14: precision: 0.571, recall: 0.308, F1: 0.400, PR AUC: 0.478\n",
            "AU_15: precision: 0.444, recall: 0.706, F1: 0.545, PR AUC: 0.762\n",
            "AU_17: precision: 0.859, recall: 0.873, F1: 0.866, PR AUC: 0.945\n",
            "AU_20: precision: 0.792, recall: 0.731, F1: 0.760, PR AUC: 0.847\n",
            "AU_23/24: precision: 0.667, recall: 0.636, F1: 0.651, PR AUC: 0.738\n",
            "AU_25: precision: 0.960, recall: 0.914, F1: 0.937, PR AUC: 0.988\n",
            "AU_26/27: precision: 0.837, recall: 0.878, F1: 0.857, PR AUC: 0.944\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.000\n",
            "AU_43: precision: 0.182, recall: 0.667, F1: 0.286, PR AUC: 0.160\n",
            "mean precision: 0.694, mean recall: 0.718, mean F1: 0.691, mean PR-AUC: 0.729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq5rtOUz7MgH"
      },
      "source": [
        "## **Third Fold**: Unfreeze FC layers and last two residual blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr0jzcQn2zch",
        "outputId": "cd058420-7daa-4511-90e6-010ccdb15ede"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 2,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint2.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if ('fc' in name) or ('layer4' in name) or ('layer3' in name):\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.5075, Time (s): 174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.2039, Val Acc: 92.1%, Time(s): 265.46\n",
            "Best Loss: Saving Model\n",
            "Epoch [2/12], Loss: 0.1639, Time (s): 441\n",
            "Epoch [2/12], Val Loss: 0.1615, Val Acc: 94.0%, Time(s): 530.23\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.1203, Time (s): 695\n",
            "Epoch [3/12], Val Loss: 0.1573, Val Acc: 93.8%, Time(s): 781.09\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.0999, Time (s): 944\n",
            "Epoch [4/12], Val Loss: 0.1510, Val Acc: 94.2%, Time(s): 1028.95\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.0784, Time (s): 1189\n",
            "Epoch [5/12], Val Loss: 0.1531, Val Acc: 94.4%, Time(s): 1272.84\n",
            "Epoch [6/12], Loss: 0.0641, Time (s): 1431\n",
            "Epoch [6/12], Val Loss: 0.1502, Val Acc: 94.3%, Time(s): 1515.18\n",
            "Best Loss: Saving Model\n",
            "Epoch [7/12], Loss: 0.0617, Time (s): 1674\n",
            "Epoch [7/12], Val Loss: 0.1516, Val Acc: 93.9%, Time(s): 1758.11\n",
            "Epoch [8/12], Loss: 0.0409, Time (s): 1917\n",
            "Epoch [8/12], Val Loss: 0.1513, Val Acc: 94.1%, Time(s): 2001.13\n",
            "Epoch [9/12], Loss: 0.0398, Time (s): 2159\n",
            "Epoch [9/12], Val Loss: 0.1468, Val Acc: 94.5%, Time(s): 2243.69\n",
            "Best Loss: Saving Model\n",
            "Epoch [10/12], Loss: 0.0290, Time (s): 2402\n",
            "Epoch [10/12], Val Loss: 0.1541, Val Acc: 94.0%, Time(s): 2486.20\n",
            "Epoch [11/12], Loss: 0.0243, Time (s): 2646\n",
            "Epoch [11/12], Val Loss: 0.1496, Val Acc: 94.3%, Time(s): 2729.78\n",
            "Epoch [12/12], Loss: 0.0210, Time (s): 2889\n",
            "Epoch [12/12], Val Loss: 0.1510, Val Acc: 94.5%, Time(s): 2972.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAX-hWrE6Yq3",
        "outputId": "65a4a1f2-5db1-4519-8099-9b0604ef3eda"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 0,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint7.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if ('fc' in name) or ('layer4' in name) or ('layer3' in name):\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.4836, Time (s): 173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.2130, Val Acc: 92.9%, Time(s): 260.89\n",
            "Best Loss: Saving Model\n",
            "Epoch [2/12], Loss: 0.1585, Time (s): 433\n",
            "Epoch [2/12], Val Loss: 0.1882, Val Acc: 93.4%, Time(s): 521.20\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.1133, Time (s): 694\n",
            "Epoch [3/12], Val Loss: 0.1748, Val Acc: 93.9%, Time(s): 781.94\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.0853, Time (s): 956\n",
            "Epoch [4/12], Val Loss: 0.1675, Val Acc: 94.2%, Time(s): 1048.15\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.0676, Time (s): 1225\n",
            "Epoch [5/12], Val Loss: 0.1674, Val Acc: 94.1%, Time(s): 1316.10\n",
            "Best Loss: Saving Model\n",
            "Epoch [6/12], Loss: 0.0518, Time (s): 1493\n",
            "Epoch [6/12], Val Loss: 0.1669, Val Acc: 94.0%, Time(s): 1583.06\n",
            "Best Loss: Saving Model\n",
            "Epoch [7/12], Loss: 0.0399, Time (s): 1759\n",
            "Epoch [7/12], Val Loss: 0.1668, Val Acc: 94.2%, Time(s): 1849.96\n",
            "Best Loss: Saving Model\n",
            "Epoch [8/12], Loss: 0.0339, Time (s): 2026\n",
            "Epoch [8/12], Val Loss: 0.1687, Val Acc: 94.2%, Time(s): 2116.84\n",
            "Epoch [9/12], Loss: 0.0262, Time (s): 2294\n",
            "Epoch [9/12], Val Loss: 0.1698, Val Acc: 94.1%, Time(s): 2383.91\n",
            "Epoch [10/12], Loss: 0.0227, Time (s): 2559\n",
            "Epoch [10/12], Val Loss: 0.1717, Val Acc: 94.2%, Time(s): 2649.67\n",
            "Epoch [11/12], Loss: 0.0202, Time (s): 2825\n",
            "Epoch [11/12], Val Loss: 0.1707, Val Acc: 94.3%, Time(s): 2914.95\n",
            "Epoch [12/12], Loss: 0.0163, Time (s): 3090\n",
            "Epoch [12/12], Val Loss: 0.1741, Val Acc: 94.3%, Time(s): 3180.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YFzQfi66kem",
        "outputId": "b18ab886-bc08-4840-e311-1cdfcf8613b0"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'gpu': True,\n",
        "    'transform': transform,\n",
        "    'fold': 1,\n",
        "    'checkpoint': '/content/drive/MyDrive/training/checkpoints/checkpoint8.pt',\n",
        "    'learn_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False \n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "convnet = get_model('/content/drive/My Drive/training/876.pt')\n",
        "\n",
        "# Freeze feature map layers\n",
        "for name, param in convnet.named_parameters():\n",
        "  if ('fc' in name) or ('layer4' in name) or ('layer3' in name):\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "train(convnet, args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Loss: 0.4703, Time (s): 180\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/12], Val Loss: 0.2179, Val Acc: 92.2%, Time(s): 262.62\n",
            "Best Loss: Saving Model\n",
            "Epoch [2/12], Loss: 0.1638, Time (s): 443\n",
            "Epoch [2/12], Val Loss: 0.1791, Val Acc: 93.1%, Time(s): 525.43\n",
            "Best Loss: Saving Model\n",
            "Epoch [3/12], Loss: 0.1152, Time (s): 707\n",
            "Epoch [3/12], Val Loss: 0.1748, Val Acc: 93.7%, Time(s): 790.81\n",
            "Best Loss: Saving Model\n",
            "Epoch [4/12], Loss: 0.0871, Time (s): 974\n",
            "Epoch [4/12], Val Loss: 0.1670, Val Acc: 93.7%, Time(s): 1056.85\n",
            "Best Loss: Saving Model\n",
            "Epoch [5/12], Loss: 0.0703, Time (s): 1241\n",
            "Epoch [5/12], Val Loss: 0.1633, Val Acc: 93.8%, Time(s): 1324.34\n",
            "Best Loss: Saving Model\n",
            "Epoch [6/12], Loss: 0.0530, Time (s): 1507\n",
            "Epoch [6/12], Val Loss: 0.1643, Val Acc: 94.1%, Time(s): 1590.69\n",
            "Epoch [7/12], Loss: 0.0420, Time (s): 1772\n",
            "Epoch [7/12], Val Loss: 0.1612, Val Acc: 93.9%, Time(s): 1855.06\n",
            "Best Loss: Saving Model\n",
            "Epoch [8/12], Loss: 0.0332, Time (s): 2037\n",
            "Epoch [8/12], Val Loss: 0.1584, Val Acc: 93.9%, Time(s): 2119.99\n",
            "Best Loss: Saving Model\n",
            "Epoch [9/12], Loss: 0.0259, Time (s): 2302\n",
            "Epoch [9/12], Val Loss: 0.1624, Val Acc: 94.1%, Time(s): 2387.44\n",
            "Epoch [10/12], Loss: 0.0203, Time (s): 2573\n",
            "Epoch [10/12], Val Loss: 0.1673, Val Acc: 94.1%, Time(s): 2657.63\n",
            "Epoch [11/12], Loss: 0.0174, Time (s): 2841\n",
            "Epoch [11/12], Val Loss: 0.1691, Val Acc: 93.9%, Time(s): 2926.78\n",
            "Epoch [12/12], Loss: 0.0139, Time (s): 3110\n",
            "Epoch [12/12], Val Loss: 0.1724, Val Acc: 94.1%, Time(s): 3194.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_cqaD8PlyZR",
        "outputId": "f09849ba-ec31-406b-94f4-382d5dd24584"
      },
      "source": [
        "print(f'Epoch 1: {np.mean([0.2039, 0.2130, 0.2179])} +- {np.std([0.2039, 0.2130, 0.2179])} ')\n",
        "print(f'Epoch 2: {np.mean([0.1615, 0.1882, 0.1791])} +- {np.std([0.1615, 0.1882, 0.1791])} ')\n",
        "print(f'Epoch 3: {np.mean([0.1573, 0.1748, 0.1748])} +- {np.std([0.1573, 0.1748, 0.1748])} ')\n",
        "print(f'Epoch 4: {np.mean([0.1510, 0.1675, 0.1670])} +- {np.std([0.1510, 0.1675, 0.1670])} ')\n",
        "print(f'Epoch 5: {np.mean([0.1531, 0.1674, 0.1633])} +- {np.std([0.1531, 0.1674, 0.1633])} ')\n",
        "print(f'Epoch 6: {np.mean([0.1502, 0.1669, 0.1643])} +- {np.std([0.1502, 0.1669, 0.1643])} ')\n",
        "print(f'Epoch 7: {np.mean([0.1516, 0.1668, 0.1612])} +- {np.std([0.1516, 0.1668, 0.1612])} ')\n",
        "print(f'Epoch 8: {np.mean([0.1513, 0.1687, 0.1584])} +- {np.std([0.1513, 0.1687, 0.1584])} ')\n",
        "print(f'Epoch 9: {np.mean([0.1468, 0.1698, 0.1624])} +- {np.std([0.1468, 0.1698, 0.1624])} ')\n",
        "print(f'Epoch 10: {np.mean([0.1541, 0.1717, 0.1673])} +- {np.std([0.1541, 0.1717, 0.1673])} ')\n",
        "print(f'Epoch 11: {np.mean([0.1496, 0.1707, 0.1691])} +- {np.std([0.1496, 0.1707, 0.0174])} ')\n",
        "print(f'Epoch 12: {np.mean([0.1510, 0.1741, 0.1724])} +- {np.std([0.1510, 0.1741, 0.1724])} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 0.2116 +- 0.00580057468417283 \n",
            "Epoch 2: 0.17626666666666668 +- 0.011082819536963006 \n",
            "Epoch 3: 0.16896666666666668 +- 0.008249579113843062 \n",
            "Epoch 4: 0.16183333333333336 +- 0.007663042621715106 \n",
            "Epoch 5: 0.16126666666666667 +- 0.006012394605220851 \n",
            "Epoch 6: 0.16046666666666667 +- 0.007336817354199903 \n",
            "Epoch 7: 0.15986666666666668 +- 0.006276587891167901 \n",
            "Epoch 8: 0.15946666666666665 +- 0.0071434507689833535 \n",
            "Epoch 9: 0.15966666666666665 +- 0.009586564672614593 \n",
            "Epoch 10: 0.16436666666666666 +- 0.007478561953269422 \n",
            "Epoch 11: 0.11256666666666666 +- 0.06784208788715813 \n",
            "Epoch 12: 0.16583333333333333 +- 0.010511686618024513 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIce6jRq-yh-",
        "outputId": "ccf0969e-6585-40cc-a9a0-d64cda4f4a64"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint7.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=0, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.912, recall: 0.897, F1: 0.904, PR AUC: 0.929\n",
            "AU_2: precision: 0.756, recall: 0.795, F1: 0.775, PR AUC: 0.887\n",
            "AU_4: precision: 0.812, recall: 0.709, F1: 0.757, PR AUC: 0.833\n",
            "AU_5: precision: 1.000, recall: 0.757, F1: 0.862, PR AUC: 0.954\n",
            "AU_6: precision: 0.718, recall: 0.700, F1: 0.709, PR AUC: 0.818\n",
            "AU_7: precision: 0.654, recall: 0.472, F1: 0.548, PR AUC: 0.583\n",
            "AU_9: precision: 0.957, recall: 0.917, F1: 0.936, PR AUC: 0.959\n",
            "AU_10: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.072\n",
            "AU_12: precision: 0.796, recall: 0.951, F1: 0.867, PR AUC: 0.936\n",
            "AU_14: precision: 0.714, recall: 0.556, F1: 0.625, PR AUC: 0.694\n",
            "AU_15: precision: 0.600, recall: 0.778, F1: 0.677, PR AUC: 0.782\n",
            "AU_17: precision: 0.864, recall: 0.905, F1: 0.884, PR AUC: 0.935\n",
            "AU_20: precision: 0.760, recall: 0.826, F1: 0.792, PR AUC: 0.918\n",
            "AU_23/24: precision: 0.684, recall: 0.481, F1: 0.565, PR AUC: 0.672\n",
            "AU_25: precision: 0.950, recall: 0.950, F1: 0.950, PR AUC: 0.982\n",
            "AU_26/27: precision: 0.884, recall: 0.844, F1: 0.864, PR AUC: 0.908\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.000\n",
            "AU_43: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.075\n",
            "mean precision: 0.670, mean recall: 0.641, mean F1: 0.651, mean PR-AUC: 0.719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv7eYGp--hbt",
        "outputId": "a5692758-9b11-421d-9362-b34a1009ea59"
      },
      "source": [
        "model = get_model('/content/drive/My Drive/training/876.pt')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint8.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=1, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.784, recall: 0.755, F1: 0.769, PR AUC: 0.889\n",
            "AU_2: precision: 0.763, recall: 0.829, F1: 0.795, PR AUC: 0.889\n",
            "AU_4: precision: 0.667, recall: 0.863, F1: 0.752, PR AUC: 0.847\n",
            "AU_5: precision: 0.786, recall: 0.917, F1: 0.846, PR AUC: 0.922\n",
            "AU_6: precision: 0.690, recall: 0.625, F1: 0.656, PR AUC: 0.794\n",
            "AU_7: precision: 0.667, recall: 0.588, F1: 0.625, PR AUC: 0.544\n",
            "AU_9: precision: 1.000, recall: 0.944, F1: 0.971, PR AUC: 0.953\n",
            "AU_10: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.179\n",
            "AU_12: precision: 0.909, recall: 0.833, F1: 0.870, PR AUC: 0.956\n",
            "AU_14: precision: 0.333, recall: 0.250, F1: 0.286, PR AUC: 0.425\n",
            "AU_15: precision: 0.963, recall: 0.684, F1: 0.800, PR AUC: 0.913\n",
            "AU_17: precision: 0.922, recall: 0.839, F1: 0.879, PR AUC: 0.965\n",
            "AU_20: precision: 0.696, recall: 0.727, F1: 0.711, PR AUC: 0.775\n",
            "AU_23/24: precision: 0.737, recall: 0.519, F1: 0.609, PR AUC: 0.764\n",
            "AU_25: precision: 0.914, recall: 0.944, F1: 0.929, PR AUC: 0.957\n",
            "AU_26/27: precision: 0.811, recall: 0.882, F1: 0.845, PR AUC: 0.939\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.016\n",
            "AU_43: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.031\n",
            "mean precision: 0.647, mean recall: 0.622, mean F1: 0.630, mean PR-AUC: 0.709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYtkhemC4F6o",
        "outputId": "084c6c46-e08a-4ce9-f2c0-5a1670ca82eb"
      },
      "source": [
        "checkpoint = torch.load('/content/drive/MyDrive/training/checkpoints/checkpoint2.pt')\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Instantiate dataloader\n",
        "dataset = AUDataset(mode='validate', fold=2, transform=transform)\n",
        "data = DataLoader(dataset, 32, collate_fn=collate_fn, shuffle=True, \n",
        "           num_workers=4)\n",
        "\n",
        "# Compute scores\n",
        "precision, recall, f1, roc_auc = get_metrics(model, data)\n",
        "for i in range(18):\n",
        "  print(f'{dataset.label_cols[i]}: precision: {precision[i]:.3f}, recall: {recall[i]:.3f}, F1: {f1[i]:.3f}, PR AUC: {roc_auc[i]:.3f}')\n",
        "print(f'mean precision: {np.mean(precision):.3f}, mean recall: {np.mean(recall):.3f}, mean F1: {np.mean(f1):.3f}, mean PR-AUC: {np.mean(roc_auc):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU_1: precision: 0.746, recall: 0.863, F1: 0.800, PR AUC: 0.897\n",
            "AU_2: precision: 0.909, recall: 0.968, F1: 0.937, PR AUC: 0.983\n",
            "AU_4: precision: 0.853, recall: 0.817, F1: 0.835, PR AUC: 0.909\n",
            "AU_5: precision: 0.700, recall: 0.724, F1: 0.712, PR AUC: 0.774\n",
            "AU_6: precision: 0.717, recall: 0.717, F1: 0.717, PR AUC: 0.858\n",
            "AU_7: precision: 0.630, recall: 0.674, F1: 0.652, PR AUC: 0.696\n",
            "AU_9: precision: 0.962, recall: 0.893, F1: 0.926, PR AUC: 0.991\n",
            "AU_10: precision: 0.500, recall: 0.400, F1: 0.444, PR AUC: 0.175\n",
            "AU_12: precision: 0.951, recall: 0.886, F1: 0.918, PR AUC: 0.962\n",
            "AU_14: precision: 0.600, recall: 0.231, F1: 0.333, PR AUC: 0.469\n",
            "AU_15: precision: 0.522, recall: 0.706, F1: 0.600, PR AUC: 0.776\n",
            "AU_17: precision: 0.915, recall: 0.857, F1: 0.885, PR AUC: 0.945\n",
            "AU_20: precision: 0.952, recall: 0.769, F1: 0.851, PR AUC: 0.871\n",
            "AU_23/24: precision: 0.765, recall: 0.591, F1: 0.667, PR AUC: 0.771\n",
            "AU_25: precision: 0.960, recall: 0.905, F1: 0.931, PR AUC: 0.990\n",
            "AU_26/27: precision: 0.881, recall: 0.902, F1: 0.892, PR AUC: 0.952\n",
            "AU_28: precision: 0.000, recall: 0.000, F1: 0.000, PR AUC: 0.000\n",
            "AU_43: precision: 0.125, recall: 0.333, F1: 0.182, PR AUC: 0.112\n",
            "mean precision: 0.705, mean recall: 0.680, mean F1: 0.682, mean PR-AUC: 0.730\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}